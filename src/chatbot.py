# rag_chatbot.py

from __future__ import annotations

import json, sys, argparse
from pathlib import Path
from typing import List, Tuple, Dict
import numpy as np
import torch
from collections import Counter
import re
import sys
sys.path.append(str(Path(__file__).resolve().parent))
from inference.llm_model import local_llm
from inference.embedding_model import embedder
from search.fine_search import fine_search_chunks
import sys
if sys.stdin.encoding != 'utf-8':
    sys.stdin.reconfigure(encoding='utf-8', errors='replace')
###############################################################################
# Utils
###############################################################################
PROJ_ROOT = Path(__file__).resolve().parents[1]

def _resolve(p: str | Path) -> Path:
    p = Path(p)
    if p.is_file():
        return p
    if (PROJ_ROOT / p).is_file():
        return PROJ_ROOT / p
    raise FileNotFoundError(p)

###############################################################################
# ë°ì´í„° ë¡œë”
###############################################################################

def load_chunks(chunks_path: str | Path, vectors_path: str | Path) -> List[dict]:
    chunks_path, vectors_path = _resolve(chunks_path), _resolve(vectors_path)

    with open(chunks_path, encoding="utf-8") as f_txt:
        chunks_raw = json.load(f_txt)
    
    with open(vectors_path, encoding="utf-8") as f_vec:
        vec_raw = json.load(f_vec)
    
    # Combine chunks with their embeddings
    chunk_index = []
    for i, (chunk, vec_item) in enumerate(zip(chunks_raw, vec_raw)):
        if isinstance(vec_item, dict):
            embedding = vec_item["embedding"]
            metadata = vec_item.get("metadata", {})
        else:
            embedding = vec_item
            metadata = {}
        
        # Get text content
        if isinstance(chunk, str):
            content = chunk
        elif isinstance(chunk, dict):
            content = chunk.get("text") or chunk.get("content")
            metadata.update(chunk.get("metadata", {}))
        else:
            raise ValueError(f"Unsupported chunk format: {type(chunk)}")
        
        chunk_index.append({
            "embedding": embedding,
            "metadata": metadata,
            "content": content
        })
    
    return chunk_index

###############################################################################
# Query refinement functions
###############################################################################

def extract_keywords(texts: List[str], top_n: int = 5) -> List[str]:
    """Extract most common meaningful words from texts."""
    # Split texts into words and filter
    words = []
    for text in texts:
        # Basic word extraction (Korean and English)
        tokens = re.findall(r'[ê°€-íž£]+|[a-zA-Z]+', text.lower())
        # Filter out short words and common stopwords
        tokens = [t for t in tokens if len(t) > 1]
        words.extend(tokens)
    
    # Count frequencies
    word_freq = Counter(words)
    
    # Return top N most common words
    return [word for word, _ in word_freq.most_common(top_n)]

def refine_query(original_query: str, search_results: List[Dict], texts: List[str]) -> str:
    """Refine query based on initial search results."""
    # Extract text from top results
    result_texts = []
    for idx in search_results:
        if 0 <= idx < len(texts):
            result_texts.append(texts[idx])
    
    # Extract keywords from results
    keywords = extract_keywords(result_texts, top_n=5)
    
    # Combine original query with extracted keywords
    # Remove duplicates while preserving order
    query_words = original_query.split()
    combined_words = query_words.copy()
    
    for keyword in keywords:
        if keyword not in original_query.lower():
            combined_words.append(keyword)
    
    # Create refined query
    refined_query = " ".join(combined_words[:15])  # Limit length
    
    return refined_query

def direct_search(query: str, chunk_index: List[dict], 
                 top_k: int = 5) -> List[dict]:
    """Perform direct search comparing query with all chunks."""
    # Encode query
    query_emb = embedder.get_embedding(query)
    
    # Use fine_search_chunks to search all chunks
    results = fine_search_chunks(query_emb, chunk_index, top_k=top_k)
    
    return results

def detect_language(text: str) -> str:
    """Detect the language of the input text."""
    # Extended language detection based on character ranges
    language_patterns = {
        'ko': r'[ê°€-íž£]',  # Korean
        'en': r'[a-zA-Z]',  # English
        'zh': r'[ä¸€-é¾¥]',  # Chinese
        'ja': r'[ã-ã‚”ã‚¡-ãƒ´ãƒ¼]',  # Japanese (Hiragana + Katakana)
        'es': r'[Ã¡Ã©Ã­Ã³ÃºÃ±ÃÃ‰ÃÃ“ÃšÃ‘]',  # Spanish
        'fr': r'[Ã Ã¢Ã§Ã¨Ã©ÃªÃ«Ã®Ã¯Ã´Ã¹Ã»Ã€Ã‚Ã‡ÃˆÃ‰ÃŠÃ‹ÃŽÃÃ”Ã™Ã›]',  # French
        'de': r'[Ã¤Ã¶Ã¼ÃŸÃ„Ã–Ãœ]',  # German
        'ru': r'[Ð°-ÑÐ-Ð¯Ñ‘Ð]',  # Russian (Cyrillic)
        'ar': r'[Ø§-ÙŠ]',  # Arabic
        'hi': r'[à¤…-à¤¹]',  # Hindi (Devanagari)
        'vi': r'[Ã Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘Ä]',  # Vietnamese
        'th': r'[à¸-à¹™]',  # Thai
        'pt': r'[Ã Ã¡Ã¢Ã£Ã§Ã©ÃªÃ­Ã³Ã´ÃµÃºÃ€ÃÃ‚ÃƒÃ‡Ã‰ÃŠÃÃ“Ã”Ã•Ãš]',  # Portuguese
        'it': r'[Ã Ã¨Ã©Ã¬Ã­Ã®Ã²Ã³Ã¹ÃºÃ€ÃˆÃ‰ÃŒÃÃŽÃ’Ã“Ã™Ãš]',  # Italian
        'tr': r'[Ã§ÄŸÄ±Ã¶ÅŸÃ¼Ã‡ÄžÄ°Ã–ÅžÃœ]',  # Turkish
        'id': r'[a-zA-Z]',  # Indonesian (uses English alphabet)
        'ms': r'[a-zA-Z]',  # Malay (uses English alphabet)
        'pl': r'[Ä…Ä‡Ä™Å‚Å„Ã³Å›ÅºÅ¼Ä„Ä†Ä˜ÅÅƒÃ“ÅšÅ¹Å»]',  # Polish
        'nl': r'[a-zA-Z]',  # Dutch (mostly English alphabet)
        'sv': r'[Ã¥Ã¤Ã¶Ã…Ã„Ã–]',  # Swedish
        'he': r'[×-×ª]',  # Hebrew
        'bn': r'[à¦…-à¦”à¦•-à¦¨à¦ª-à¦¯à¦¼]',  # Bengali
        'ta': r'[à®…-à®¹]',  # Tamil
        'te': r'[à°…-à°¹]',  # Telugu
        'mr': r'[à¤…-à¤¹]',  # Marathi
    }
    
    char_counts = {}
    
    # Count characters for each language
    for lang, pattern in language_patterns.items():
        chars = re.findall(pattern, text)
        char_counts[lang] = len(chars)
    
    # Special handling for languages that use English alphabet
    if char_counts['en'] > 0:
        # Check for specific words or patterns
        text_lower = text.lower()
        if any(word in text_lower for word in ['adalah', 'yang', 'dan', 'untuk', 'dengan']):
            char_counts['id'] = char_counts['en'] + 10  # Indonesian
        elif any(word in text_lower for word in ['yang', 'dan', 'untuk', 'adalah', 'dengan']):
            char_counts['ms'] = char_counts['en'] + 10  # Malay
        elif any(word in text_lower for word in ['het', 'een', 'van', 'voor', 'met']):
            char_counts['nl'] = char_counts['en'] + 10  # Dutch
    
    # Return the language with most characters
    if max(char_counts.values()) == 0:
        return 'ko'  # Default to Korean
    
    return max(char_counts, key=char_counts.get)

def get_language_instruction(lang: str) -> str:
    """Get language-specific instruction for the response."""
    instructions = {
        'ko': 'í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.',
        'en': 'Please answer in English.',
        'zh': 'è¯·ç”¨ä¸­æ–‡å›žç­”ã€‚',
        'ja': 'æ—¥æœ¬èªžã§ç­”ãˆã¦ãã ã•ã„ã€‚',
        'es': 'Por favor responde en espaÃ±ol.',
        'fr': 'Veuillez rÃ©pondre en franÃ§ais.',
        'de': 'Bitte antworten Sie auf Deutsch.',
        'ru': 'ÐŸÐ¾Ð¶Ð°Ð»ÑƒÐ¹ÑÑ‚Ð°, Ð¾Ñ‚Ð²ÐµÑ‚ÑŒÑ‚Ðµ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ.',
        'ar': 'Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.',
        'hi': 'à¤•à¥ƒà¤ªà¤¯à¤¾ à¤¹à¤¿à¤‚à¤¦à¥€ à¤®à¥‡à¤‚ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥‡à¤‚à¥¤',
        'vi': 'Vui lÃ²ng tráº£ lá»i báº±ng tiáº¿ng Viá»‡t.',
        'th': 'à¸à¸£à¸¸à¸“à¸²à¸•à¸­à¸šà¹€à¸›à¹‡à¸™à¸ à¸²à¸©à¸²à¹„à¸—à¸¢',
        'pt': 'Por favor, responda em portuguÃªs.',
        'it': 'Si prega di rispondere in italiano.',
        'tr': 'LÃ¼tfen TÃ¼rkÃ§e cevap verin.',
        'id': 'Silakan jawab dalam bahasa Indonesia.',
        'ms': 'Sila jawab dalam bahasa Melayu.',
        'pl': 'ProszÄ™ odpowiedzieÄ‡ po polsku.',
        'nl': 'Antwoord alstublieft in het Nederlands.',
        'sv': 'VÃ¤nligen svara pÃ¥ svenska.',
        'he': '×× × ×¢× ×” ×‘×¢×‘×¨×™×ª.',
        'bn': 'à¦¦à¦¯à¦¼à¦¾ à¦•à¦°à§‡ à¦¬à¦¾à¦‚à¦²à¦¾à¦¯à¦¼ à¦‰à¦¤à§à¦¤à¦° à¦¦à¦¿à¦¨à¥¤',
        'ta': 'à®¤à®¯à®µà¯à®šà¯†à®¯à¯à®¤à¯ à®¤à®®à®¿à®´à®¿à®²à¯ à®ªà®¤à®¿à®²à®³à®¿à®•à¯à®•à®µà¯à®®à¯.',
        'te': 'à°¦à°¯à°šà±‡à°¸à°¿ à°¤à±†à°²à±à°—à±à°²à±‹ à°¸à°®à°¾à°§à°¾à°¨à°‚ à°‡à°µà±à°µà°‚à°¡à°¿.',
        'mr': 'à¤•à¥ƒà¤ªà¤¯à¤¾ à¤®à¤°à¤¾à¤ à¥€à¤¤ à¤‰à¤¤à¥à¤¤à¤° à¤¦à¥à¤¯à¤¾.'
    }
    return instructions.get(lang, 'í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.')

###############################################################################
# Main chat loop
###############################################################################
def answer(q, chunk_index, top_k):
    results = direct_search(q, chunk_index, top_k)
            
    ctx = []
    for rank, result in enumerate(results, 1):
        score = result.get("score", 0)
        content = result.get("content", "")
        ctx.append(f"[{rank}] (S={score:.3f})\n{content}\n")
    
    # Detect language of the question
    lang = detect_language(q)
    lang_instruction = get_language_instruction(lang)
    
    # Create a more sophisticated prompt for Sogang University assistant
    prompt = f"""ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµ í•™ìƒë“¤ì„ ìœ„í•œ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ë„ìš°ë¯¸ìž…ë‹ˆë‹¤. 
í•™ìƒë“¤ì˜ ìº í¼ìŠ¤ ìƒí™œ, í•™ì—…, í–‰ì • ì ˆì°¨ ë“± ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
ì œê³µëœ ë¬¸ë§¥ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ë˜, ë¬¸ë§¥ì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
ë‹µë³€ì€ ëª…í™•í•˜ê³  êµ¬ì²´ì ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

ì¤‘ìš”í•œ ê·œì¹™:
- ë¬¸ë§¥ ë²ˆí˜¸ë‚˜ ì¶œì²˜ë¥¼ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš” (ì˜ˆ: "[ë¬¸ë§¥ 1]ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìžˆìŠµë‹ˆë‹¤" ê°™ì€ í‘œí˜„ ê¸ˆì§€)
- "ë” ìžì„¸í•œ ë‚´ìš©ì€ ~ì—ì„œ í™•ì¸í•˜ì„¸ìš”" ê°™ì€ ë¶ˆí•„ìš”í•œ ì•ˆë‚´ëŠ” í•˜ì§€ ë§ˆì„¸ìš”
- ë‹µë³€ì— í•„ìš”í•œ ì •ë³´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì œê³µí•˜ì„¸ìš”

ë¬¸ë§¥:
{"\n".join(ctx)}

ì§ˆë¬¸: {q}

{lang_instruction} ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” í†¤ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.

ë‹µë³€:"""

    # Generate answer using local_llm
    full_response = local_llm.generate(prompt)
    # Extract only the model's response
    ans = local_llm.strip_response(full_response)
    return ans
    
def chat_loop(args: argparse.Namespace):
    print("[INFO] Loading data â€¦")
    chunk_index = load_chunks(args.chunks, args.vectors)
    print("[READY] Type your question â€” Ctrl+C to exit\n")
    print("[INFO] Using direct search to compare with all chunks\n")
    try:
        while True:
            q = input("ðŸ—¨ï¸  Q: ").strip()
            if not q:
                continue

            # Use direct search
            print("ðŸ” Searching all chunks...")
            ans = answer(q, chunk_index, args.topk)
            print("ðŸ¤– A:", ans, "\n")
    except KeyboardInterrupt:
        print("\n[EXIT]")

###############################################################################
# CLI
###############################################################################

def parse_args(argv: List[str] | None = None) -> argparse.Namespace:
    p = argparse.ArgumentParser()
    p.add_argument("--chunks", default="data/chunks/pages_text_chunks.json")
    p.add_argument("--vectors", default="data/index/pages_text_chunks_vectors.json")
    p.add_argument("--device", choices=["cpu", "cuda", "mps"], default="cpu")
    p.add_argument("--topk", type=int, default=5)
    return p.parse_args(argv)

###############################################################################
# Entrypoint
###############################################################################
if __name__ == "__main__":
    chat_loop(parse_args())